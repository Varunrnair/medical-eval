{
  "dataSources": [
    {
      "id": "scored-final-dataset",
      "name": "Medical QA Evaluation Results",
      "description": "Primary evaluation dataset with medical, semantic, and linguistic scores",
      "filePath": "/scored_final_dataset.csv",
      "schema": {
        "Theme": "string",
        "Questions": "string",
        "Answer": "text",
        "Hindi": "string",
        "Marathi": "string",
        "References": "string",
        "llm_response": "string",
        "bleu_score": "number",
        "meteor_score": "number",
        "rouge_l_score": "number",
        "perplexity": "number",
        "linguistic_quality_score": "number",
        "language": "string",
        "sbert_similarity": "number",
        "vyakyarth_similarity": "number",
        "distiluse_similarity": "number",
        "labse_similarity": "number",
        "cohere_similarity": "number",
        "voyage_similarity": "number",
        "openai_similarity": "number",
        "bert_score_f1": "number",
        "semantic_similarity": "number",
        "medical_quality_score": "number",
        "medical_quality_score_2": "number"
      },
      "visualizations": ["pie", "radar", "bar", "metrics"],
      "lastUpdated": "2024-01-01T00:00:00Z"
    },
    {
      "id": "medical-quality-detailed",
      "name": "Medical Quality Detailed Scores",
      "description": "Detailed breakdown of medical quality evaluation with rubric scores",
      "filePath": "/medical_quality_detailed_scores.csv",
      "schema": {
        "question": "string",
        "gold_standard_answer": "string",
        "llm_response": "string",
        "rubrics": "string",
        "rubric_scores": "string",
        "classification": "string",
        "axis_scores": "string",
        "medical_quality_score": "number"
      },
      "visualizations": ["radar", "bar"],
      "lastUpdated": "2024-01-01T00:00:00Z"
    },
    {
      "id": "medical-2-quality-detailed",
      "name": "Medical 2 Quality Detailed Scores",
      "description": "Detailed breakdown of medical 2 quality evaluation with rubric scores",
      "filePath": "/medical_2_quality_detailed_scores.csv",
      "schema": {
        "question": "string",
        "gold_standard_answer": "string",
        "llm_response": "string",
        "generated_rubrics": "string",
        "fixed_rubrics": "string",
        "all_rubrics": "string",
        "rubric_scores": "string",
        "classification": "string",
        "axis_scores": "string",
        "medical_quality_score": "number"
      },
      "visualizations": ["radar", "bar"],
      "lastUpdated": "2024-01-01T00:00:00Z"
    },
    {
      "id": "linguistic-detailed",
      "name": "Linguistic Quality Scores",
      "description": "Average linguistic quality metrics including BLEU, METEOR, and ROUGE scores",
      "filePath": "/linguistic_detailed_scores.csv",
      "schema": {
        "avg_bleu_score": "number",
        "avg_meteor_score": "number",
        "avg_rouge_l_score": "number",
        "avg_perplexity": "number",
        "avg_linguistic_quality_score": "number"
      },
      "visualizations": ["bar", "metrics"],
      "lastUpdated": "2024-01-01T00:00:00Z"
    },
    {
      "id": "semantic-detailed",
      "name": "Semantic Quality Scores",
      "description": "Average semantic similarity metrics including SBERT, Vyakyarth, DistilUSE, LaBSE, Cohere, Voyage, OpenAI, and BERTScore",
      "filePath": "/semantic_detailed_scores.csv",
      "schema": {
        "avg_sbert_similarity": "number",
        "avg_vyakyarth_similarity": "number",
        "avg_distiluse_similarity": "number",
        "avg_labse_similarity": "number",
        "avg_cohere_similarity": "number",
        "avg_voyage_similarity": "number",
        "avg_openai_similarity": "number",
        "avg_bert_score_f1": "number",
        "avg_semantic_similarity": "number"
      },
      "visualizations": ["bar", "metrics"],
      "lastUpdated": "2024-01-01T00:00:00Z"
    },
    {
      "id": "medical-3-detailed",
      "name": "Medical 3 Detailed Scores",
      "description": "Detailed breakdown of medical 3 evaluation with rubric and theme scores",
      "filePath": "/medical_3/scored_dataset_detailed.csv",
      "schema": {
        "index": "number",
        "Theme": "string",
        "question": "string",
        "llm_response": "string",
        "medical_quality_score": "number",
        "axis_scores": "object",
        "rubric_counts_by_axis": "object",
        "rubric_scores_by_axis": "object",
        "all_rubric_scores_flat": "object",
        "Accuracy_score": "number",
        "Accuracy_rubric_count": "number",
        "Completeness_score": "number",
        "Completeness_rubric_count": "number",
        "Context Awareness_score": "number",
        "Context Awareness_rubric_count": "number",
        "Communication_score": "number",
        "Communication_rubric_count": "number",
        "Terminology Accessibility_score": "number",
        "Terminology Accessibility_rubric_count": "number"
      },
      "visualizations": ["bar", "radar", "metrics"],
      "lastUpdated": "2024-01-01T00:00:00Z"
    }
  ]
}
